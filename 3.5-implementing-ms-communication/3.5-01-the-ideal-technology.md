# 3.5-01-The Ideal Technology

## SLIDE 3.5-01-01

La scelta della tecnologia dovrebbe essere guidata in gran parte dallo stile di comunicazione desiderato. Decidere tra chiamate _sincrone bloccanti_ o _asincrone non bloccanti_, collaborazione di tipo _richiesta-risposta_ o basata sugli _eventi_, ci aiuterà a ridurre la lista delle possibili tecnologie, che altrimenti potrebbe essere molto lunga. Cerchiamo di esaminare alcune delle tecnologie comunemente utilizzate per la comunicazione tra microservizi.

Esiste una confusione di opzioni su come un microservizio possa comunicare con un altro e scegliere non è semplice: SOAP, XML-RPC, REST, gRPC? E continuano sempre a emergere nuove opzioni. Prima di discutere tecnologie specifiche, riflettiamo su ciò che vogliamo realmente ottenere dalla tecnologia che scegliamo.

**Retrocompatibilità** Quando apportiamo modifiche ai nostri microservizi, dobbiamo assicurarci di non compromettere la compatibilità con i microservizi consumers. Pertanto, vogliamo garantire che qualsiasi tecnologia che scegliamo renda semplice apportare modifiche retrocompatibili. Operazioni semplici come l'aggiunta di nuovi campi non dovrebbero interrompere il funzionamento dei client. Idealmente, vogliamo anche la capacità di convalidare che le modifiche apportate siano retrocompatibili e avere un modo per ottenere un feedback in tal senso prima di distribuire il microservizio in produzione.

Rendere **Esplicita** la Tua Interfaccia ciò significa che deve essere chiaro per un consumer di quel microservizio quale funzionalità il microservizio espone. Ma significa anche che deve essere chiaro per uno sviluppatore che lavora sul microservizio quale funzionalità deve rimanere intatta per le parti esterne.

Mantenere le Tue API **Agnostiche** dalla Tecnologia: nuovi strumenti, framework e linguaggi emergono costantemente, implementando nuove idee che possono aiutarci a lavorare più velocemente ed in modo più efficace. In questo momento, potresti essere orientato su .NET o su Java. Ma cosa succederà fra un anno o cinque anni? E se volessi sperimentare con un'alternativa di stack tecnologico che potrebbe renderti più produttivo? Qui ci aiutano molto i microservizi. Ciò significa valutare attentamente se usare tecnologie di integrazione che impongono quali stack tecnologici possiamo utilizzare per implementare i nostri microservizi.

Rendi il Tuo Servizio **Semplice** per i consumers perché avere un microservizio ben strutturato non conta molto se il costo di utilizzo per i consumatori è elevato! Idealmente, vorremmo concedere ai nostri client piena libertà nella scelta della tecnologia; d'altra parte, fornire una libreria per il client può agevolare l'adozione.

**Nascondi i Dettagli** di Implementazione Interna perché ciò porta ad un aumento dell'accoppiamento; ciò significa che se vogliamo cambiare qualcosa all'interno del nostro microservizio, possiamo interrompere i consumatori richiedendo loro di apportare anche cambiamenti. Questo aumenta il costo del cambiamento, esattamente ciò che stiamo cercando di evitare.&#x20;

## SLIDE 3.5-01-02

Esistono numerose tecnologie che potremmo esaminare vediamo alcune delle scelte più popolari e interessanti. Queste sono le opzioni che esamineremo:&#x20;

* Chiamate di **procedura** remote Framework che consente di invocare chiamate di metodo locali su un processo remoto. Le opzioni comuni includono SOAP e gRPC.
* **REST** Uno stile architetturale in cui si espongono risorse (Cliente, Ordine, ecc.) che possono essere accessate utilizzando un insieme comune di verbi (GET, POST)
* **GraphQL**, un protocollo relativamente recente che consente ai consumers di definire query personalizzate in grado di recuperare informazioni da vari microservizi, filtrando i risultati per restituire solo ciò che è necessario
* **Message Broker** Middleware: consente la comunicazione asincrona tramite code o argomenti (topics).

Vediamo queste opzione un po' più nel dettaglio

## SLIDE 3.5-01-03

Il termine "**Remote Procedure Call**" (RPC) si riferisce alla tecnica di effettuare una chiamata locale e farla eseguire su un servizio remoto in un luogo diverso e ne esistono diverse implementazioni.

La maggior parte delle tecnologie in questo contesto richiede uno schema esplicito, come ad esempio SOAP o gRPC. Nel contesto dell'RPC, lo schema viene spesso definito come linguaggio di definizione dell'interfaccia (IDL), con SOAP che si riferisce al suo formato di schema come linguaggio di definizione del servizio web (WSDL).&#x20;

L'uso di uno schema separato facilita la generazione di stub client e server per diversi stack tecnologici. Ad esempio, potrei avere un server Java che espone un'interfaccia SOAP e un client .NET generato dalla stessa definizione WSDL dell'interfaccia. Altre tecnologie, come Java RMI, richiedono un'accoppiamento un po' più stretto tra il client e il server ma comunque tutte queste tecnologie hanno la stessa caratteristica principale: _rendono una chiamata remota simile a una chiamata locale._

Nota, di solito l'uso di una tecnologia RPC implica l'adozione di un protocollo di _serializzazione_

Il fatto che i framework RPC con uno schema esplicito rendono molto semplice la generazione del codice lato client ci evita potenzialmente la necessità di librerie client, poiché qualsiasi client può semplicemente generare il proprio codice in base a questa specifica di servizio.&#x20;

Tuttavia, affinché la generazione del codice lato client funzioni, il client ha bisogno di un modo per ottenere lo schema in modo asincrono, ovvero il consumatore deve avere accesso allo schema prima di pianificare le chiamate.&#x20;

Questa tecnologia non è chiaramente priva di svantaggi e alcune implementazioni dell'RPC possono presentare più problemi di altre:&#x20;

**Accoppiamento tecnologico**: alcuni meccanismi RPC, come Java RMI, sono strettamente legati a una piattaforma specifica, il che può limitare quale tecnologia può essere utilizzata nel client e nel server. In un certo senso, questo accoppiamento tecnologico può essere una forma di esposizione di dettagli di implementazione tecnica interna. Ad esempio, l'uso di RMI lega non solo il client alla JVM ma anche il server

Le **chiamate locali** non sono come le chiamate _remote_. L'idea centrale dell'RPC è nascondere la complessità di una chiamata remota ma questo può portare a nascondere troppi dettagli. Con l'RPC il costo della serializzazione e deserializzazione dei carichi può essere significativo, senza considerare il tempo necessario per inviare i dati  attraverso la rete. Ciò significa che è necessario pensare in modo diverso alla progettazione dell'API per le interfacce remote rispetto alle interfacce locali.&#x20;

È necessario considerare la rete stessa perché di base le reti non sono affidabili, possono e falliranno, anche se il client e il server con cui stai comunicando sono in buone condizioni. Possono fallire rapidamente, possono fallire lentamente e possono persino degradare le comunicazioni.&#x20;

**Fragilità**. Alcune delle implementazioni più popolari di RPC possono portare a forme non previste di fragilità ed è un ottimo esempio è _Java RMI_. Consideriamo una semplice interfaccia Java che abbiamo deciso di rendere un'API remota per il nostro servizio Cliente, Java RMI genera gli stub client e server per il nostro metodo. Ma cosa succede se decidiamo di aggiungere nuovi metodi?

Il problema è che ora dobbiamo rigenerare anche gli stub client. I client che desiderano consumare i nuovi metodi hanno bisogno dei nuovi stub e, a seconda della natura delle modifiche alla specifica, i consumers che non necessitano del nuovo metodo potrebbero comunque dover aggiornare i loro stub.

La realtà è che modifiche come queste sono piuttosto comuni e sono gestibili fino ad un certo perché gli endpoint RPC finiscono spesso per avere un gran numero di metodi per diverse modalità di creazione o interazione con gli oggetti. Questo è in parte dovuto al fatto che continuiamo a pensare a queste chiamate remote come se fossero locali.

Cosa succederebbe se scopriamo che, anche se esponiamo il campo età nei nostri oggetti Customer, nessuno dei nostri consumatori lo utilizza mai? Decidiamo di voler rimuovere questo campo. Ma se l'implementazione del server rimuove l'età dalla definizione di questo tipo e noi non facciamo lo stesso per tutti i consumatori, allora anche se non hanno mai usato il campo, il codice associato alla deserializzazione dell'oggetto Customer sul lato del consumatore si romperà.

Alcune implementazioni presentano problemi un po' più significativi di altre, ad esempio SOAP è piuttosto pesante dal punto di vista dello sviluppatore, specialmente se confrontato con scelte più moderne. Bisogna fare attenzione ad alcuni dei potenziali rischi associati all'RPC se stiamo scegliendo questo modello. Non astrarre le chiamate remote al punto che la rete sia completamente nascosta e assicuratevi di poter evolvere l'interfaccia del server senza dover insistere su aggiornamenti sincronizzati per i client.&#x20;

Trovare l'equilibrio giusto per il codice client è importante e non sempre banale, assicuratevi che i client non siano completamente ignari del fatto che verrà effettuata una chiamata di rete.&#x20;

## SLIDE 3.5-01-04

**Representational State Transfer (REST)** è uno stile architetturale ispirato al web abbiamo molte idee di design e vincoli dietro lo stile REST, ma ci concentreremo su quelli che ci aiutano veramente quando ragioniamo nel mondo dei microservizi e quando cerchiamo un'alternativa all'RPC per le interfacce dei nostri servizi.&#x20;

Il concetto più importante da considerare quando si pensa a REST è quello delle _risorse_. Puoi pensare a una risorsa come a una cosa di cui il servizio stesso è a conoscenza, come un Cliente. Il server crea diverse rappresentazioni di questo Cliente su richiesta.&#x20;

Come una risorsa viene mostrata esternamente è completamente disaccoppiata da come viene memorizzata internamente. Ad esempio, un client potrebbe richiedere una rappresentazione JSON di un Cliente, anche se è memorizzato in un formato completamente diverso. Una volta che un client ha una rappresentazione di questo Cliente, può quindi effettuare richieste per modificarlo e il server potrebbe o meno conformarsi a tali richieste.&#x20;

REST di per sé non parla davvero dei protocolli sottostanti, anche se è più comunemente usato su HTTP che semplifica l'implementazione. Questo perché HTTP stesso definisce alcune capacità utili che si adattano molto bene allo stile REST. Ad esempio, i **verbi** HTTP (ad esempio, GET, POST e PUT) hanno già significati ben compresi nelle specifiche di HTTP su come dovrebbero funzionare con le risorse. Lo stile architetturale REST ci dice effettivamente che questi verbi dovrebbero comportarsi allo stesso modo su tutte le risorse, e la specifica di HTTP definisce un insieme di verbi che possiamo utilizzare. GET recupera una risorsa in modo idempotente, ad esempio, e POST crea una nuova risorsa.&#x20;

HTTP porta anche un ampio ecosistema di strumenti e tecnologie di supporto. Possiamo utilizzare proxy di caching HTTP, bilanciatori di carico e molti strumenti di monitoraggio che hanno già un sacco di supporto per HTTP

Nota che HTTP può essere utilizzato anche per altre tecnologie. Ad esempio, SOAP viene instradato su HTTP, ma purtroppo utilizza pochissimo delle specifiche. I verbi vengono ignorati, così come semplici cose come i codici di errore HTTP.&#x20;

Un altro principio introdotto in REST che può aiutarci a evitare l'accoppiamento tra client e server è il concetto di "**hypermedia come motore dello stato dell'applicazione**" (spesso abbreviato come _HATEOAS_).&#x20;

L'hypermedia è un concetto in cui un pezzo di contenuto contiene collegamenti ad altri pezzi di contenuto in vari formati (ad esempio, testo, immagini, suoni), se ci pensate è ciò che accade nella pagina web media: segui i collegamenti, che sono una forma di controlli hypermedia, per visualizzare contenuti correlati.&#x20;

L'idea dietro _HATEOAS_ è che i client dovrebbero eseguire interazioni con il server (potenzialmente portando a transizioni di stato) tramite questi collegamenti ad altre risorse. Un client non ha bisogno di sapere esattamente dove risiedono i client sul server conoscendo quale URI navigare; invece, il client cerca e naviga tra i collegamenti per trovare ciò di cui ha bisogno.

#### Svantaggi

Dal punto di vista della facilità di utilizzo, non potresti facilmente generare codice lato client per il tuo protocollo di applicazione REST su HTTP come potresti fare con le implementazioni RPC. Questo ha spesso portato le persone a creare API REST che forniscono librerie client per i conumsers da utilizzare.&#x20;

Queste librerie client ci offrono un collegamento all'API per semplificare l'integrazione del client ma Il problema è che le librerie client possono nascondere alcune sfide riguardo all'accoppiamento tra il client e il server.

Negli ultimi anni questo problema è stato in parte attenuato. La specifica OpenAPI che è nata dal progetto Swagger ora ti consente di definire informazioni sufficienti su un endpoint REST per consentire la generazione di codice lato client in una varietà di linguaggi.

Le prestazioni potrebbero anche rappresentare un problema. I carichi utilizzati da REST su HTTP possono essere effettivamente più compatti rispetto a SOAP poiché REST supporta formati alternativi come JSON o persino binari, ma comunque non saranno mai così snelli come potrebbe essere un protocollo binario.

L'overhead di HTTP per ogni richiesta potrebbe essere un problema per i requisiti di bassa latenza considerando che tutti i protocolli HTTP principali attualmente in uso richiedono l'uso del protocollo di controllo di trasmissione (TCP) sotto ai piedi, il quale ha inefficienze rispetto ad altri protocolli di rete, e alcune implementazioni RPC ti consentono di utilizzare protocolli di rete alternativi al TCP come il protocollo UDP.

Da considerare che le limitazioni imposte a HTTP stanno venendo affrontate e attualmente HTTP/3 in fase di finalizzazione, sta cercando di passare all'utilizzo di protocolli più snelli mantenendo le stesse capacità.

A causa della sua ampia diffusione nell'industria, un'API basata su REST-over-HTTP è una scelta ovvia per un'interfaccia di tipo richiesta-risposta sincrona e consideriamo che è uno stile di interfaccia ampiamente compreso con cui la maggior parte delle persone è familiare e garantisce l'interoperabilità tra un'enorme varietà di tecnologie.

In gran parte grazie alle capacità di HTTP e al fatto che REST si basa su queste capacità, le API basate su REST eccellono nelle situazioni in cui desideri una cache di richieste su larga scala ed efficace. È per questa ragione che sono la scelta ovvia per esporre API a parti esterne o interfacce client. Tuttavia, potrebbero soffrire in confronto a protocolli di comunicazione più efficienti, e sebbene tu possa costruire protocolli di interazione asincrona sopra le API basate su REST, ciò non è davvero una soluzione ideale rispetto alle alternative per la comunicazione generale tra microservizi.

## SLIDE 3.5-01-05

Negli ultimi anni, GraphQL ha guadagnato sempre più popolarità, in gran parte perché eccelle in un'area specifica, consente a un dispositivo lato client di definire query che evitano la necessità di effettuare molteplici richieste per recuperare le stesse informazioni.&#x20;

Questo può offrire miglioramenti significativi in termini di prestazioni per dispositivi lato client con risorse limitate e può anche evitare la necessità di implementare aggregazioni personalizzate lato server. Per fare un semplice esempio, immagina un dispositivo mobile che desidera visualizzare una pagina che mostri una panoramica degli ultimi ordini di un cliente.&#x20;

La pagina deve contenere alcune informazioni sul cliente, insieme alle informazioni sui cinque ordini più recenti del cliente. Lo schermo ha bisogno solo di alcuni campi dal record del cliente e solo della data, del valore e dello stato di spedizione di ciascun ordine. Il dispositivo mobile potrebbe effettuare chiamate a due microservizi  per recuperare le informazioni necessarie, ma ciò comporterebbe molteplici chiamate, incluso il recupero di informazioni che in realtà non sono necessarie. Soprattutto con i dispositivi mobili, ciò può essere spreco: utilizza più del piano dati del dispositivo mobile di quanto sia necessario e può richiedere più tempo.

**GraphQL** consente al dispositivo mobile di emettere una singola query che può recuperare tutte le informazioni necessarie. Per farlo, è necessario un microservizio che esponga un endpoint GraphQL al dispositivo client che rappresenta l'ingresso per tutte le query del client e espone uno schema da utilizzare per i dispositivi client. Questo schema espone i tipi disponibili per il client, ed è disponibile anche un costruttore di query grafiche per semplificare la creazione di queste query. Riducendo il numero di chiamate e la quantità di dati recuperati dal dispositivo client, è possibile gestire in modo efficace alcune delle sfide che si presentano nella costruzione di interfacce utente con architetture a microservizi.

In confronto alle normali API REST basate su HTTP, la memorizzazione nella cache è anche più complessa. Con le API basate su REST, posso impostare uno dei tanti header di risposta per aiutare i dispositivi lato client o le cache intermedie come le reti di distribuzione dei contenuti (CDN) a memorizzare in cache le risposte in modo che non sia necessario richiederle nuovamente. Questo non è possibile allo stesso modo con GraphQL.&#x20;

La memorizzazione nella cache sembra essere stata ignorata consapevolmente o inconsapevolmente come parte dello sviluppo iniziale di GraphQL. Se le query che emetti sono molto specifiche per un determinato utente, allora questa mancanza di memorizzazione a livello di richiesta potrebbe non essere un fattore decisivo, naturalmente, poiché il tuo rapporto di cache-hit è probabilmente basso.&#x20;

Un'altra problematica è che sebbene GraphQL possa teoricamente gestire le scritture, sembra non essere altrettanto adatto per le operazioni di scrittura come per quelle di lettura. Questo porta a situazioni in cui i team utilizzano GraphQL per la lettura, ma REST per le scritture.

Quindi fondamentalmente, GraphQL è un meccanismo di aggregazione e filtraggio delle chiamate, quindi nel contesto di un'architettura a microservizi verrebbe utilizzato per aggregare chiamate su più microservizi downstream. Come tale, non è qualcosa che sostituirebbe la comunicazione generale tra microservizi.

## SLIDE 3.5-01-06

<figure><img src="../.gitbook/assets/Screenshot 2023-08-22 alle 22.20.05.png" alt=""><figcaption></figcaption></figure>

I Message Brokers, spesso chiamati _middleware_, sono intermediari che si collocano tra i processi per gestire la comunicazione tra di essi. Sono una scelta popolare per implementare la comunicazione asincrona tra microservizi, in quanto offrono una varietà di funzionalità interessanti.

Come abbiamo discusso in precedenza, un messaggio è un concetto generico che definisce ciò che un intermediario di messaggistica invia. Un messaggio potrebbe contenere una richiesta, una risposta o un evento. Invece che un microservizio comunichi direttamente con un altro microservizio, il microservizio fornisce un messaggio a un intermediario di messaggistica, con informazioni su come il messaggio dovrebbe essere inviato.

Gli intermediari di solito forniscono code o topic, o entrambi. Le code sono tipicamente punto a punto dove un mittente inserisce un messaggio in una coda e un consumatore ne legge dalla coda. In un sistema basato su argomenti, più consumatori possono sottoscrivere un argomento e ogni consumatore sottoscritto riceverà una copia di quel messaggio.

Un consumer potrebbe rappresentare uno o più microservizi, generalmente modellati come un gruppo. Questo sarebbe utile quando hai più istanze di un microservizio e desideri che una qualsiasi di esse possa ricevere un messaggio. In figura vediamo un esempio in cui il Processore di Ordini ha tre istanze distribuite, tutte parte dello stesso gruppo. Quando un messaggio viene inserito nella coda, solo un membro del gruppo di consumatori riceverà quel messaggio; questo significa che la coda funziona come un meccanismo di distribuzione del carico.

## SLIDE 3.5-01-07

Con il meccanismo di topics, è possibile avere più gruppi di consumers. In figura un evento che rappresenta un ordine pagato viene inserito nell'argomento "Stato dell'Ordine". Una copia di quell'evento viene ricevuta sia dal microservizio di Magazzino che dal microservizio di Notifiche, che si trovano in gruppi di consumatori separati. Solo un'istanza di ciascun gruppo di consumatori però vedrà quell'evento.

A prima vista, una coda sembra solo un argomento con un singolo gruppo di consumatori. Gran parte della distinzione tra i due è che quando un messaggio viene inviato su una coda, si sa a chi è destinato il messaggio. Con un argomento, questa informazione è nascosta al mittente del messaggio: il mittente non sa chi (se qualcuno) riceverà il messaggio alla fine.

Generalmente parlando i topics sono adatti per la collaborazione basata sugli eventi, mentre le code sarebbero più appropriate per la comunicazione di richiesta/risposta. Considerate però che ciò dovrebbe essere considerata come una guida generale piuttosto che una regola rigorosa.

Quindi, perché usare un intermediario? Di base forniscono alcune capacità che possono essere molto utili per la comunicazione asincrona. Le proprietà che offrono variano, ma la caratteristica più interessante è quella della consegna garantita, una cosa che tutti gli intermediari ampiamente utilizzati supportano in qualche modo. La consegna garantita descrive un impegno dell'intermediario a garantire che il messaggio venga consegnato.

Dal punto di vista del microservizio che invia il messaggio, questo può essere molto utile. Non è un problema se la destinazione a valle non è disponibile in quell'istante: l'intermediario conserverà il messaggio fino a quando potrà essere consegnato. Questo può ridurre il numero di cose di cui un microservizio a monte deve preoccuparsi. Confrontalo con una chiamata diretta sincrona, ad esempio una richiesta HTTP: se la destinazione a valle non è raggiungibile, il microservizio a monte dovrà decidere cosa fare con la richiesta; dovrebbe riprovare la chiamata o rinunciare?

Perché la consegna garantita funzioni, un broker deve assicurarsi che tutti i messaggi non ancora consegnati vengano mantenuti in modo durevole finché possono essere consegnati. Per mantenere questa promessa, un intermediario funzionerà di solito come un sistema basato su cluster, garantendo che la perdita di una singola macchina non comporti la perdita di un messaggio. Solitamente c'è molto coinvolto nell'esecuzione corretta di un intermediario, in parte a causa delle sfide nel gestire il software basato su cluster.&#x20;

Oltre alla consegna garantita, gli intermediari possono offrire altre caratteristiche che in genere tornano utili. La maggior parte degli intermediari può garantire l'ordine in cui i messaggi verranno consegnati, ma questo non è universale e anche in questo caso la portata di questa garanzia può essere limitata.&#x20;

Con Kafka, ad esempio, l'ordinamento è garantito solo all'interno di una singola partizione. Se non puoi essere certo che i messaggi verranno ricevuti in ordine, il tuo consumatore potrebbe dover compensare, magari ritardando l'elaborazione dei messaggi ricevuti fuori sequenza fino a quando non vengono ricevuti i messaggi mancanti.&#x20;

Alcuni intermediari forniscono transazioni in scrittura, ad esempio Kafka ti consente di scrivere su più argomenti in una singola transazione.&#x20;

Un'altra caratteristica promessa da alcuni intermediari è quella della consegna esattamente una volta. Uno dei modi più semplici per garantire la consegna garantita è consentire il ritrasmissione del messaggio. Ciò può comportare che un consumatore veda lo stesso messaggio più di una volta. La maggior parte degli intermediari farà il possibile per ridurre la possibilità di ciò o nascondere questo fatto al consumatore, ma alcuni intermediari vanno oltre garantendo una consegna esattamente una volta.&#x20;

## SLIDE 3.5-01-08

Kafka è degno di nota come intermediario specifico, in gran parte grazie alla sua popolarità recente dovuta in parte all'uso di Kafka nel movimento di grandi volumi di dati nell'ambito dell'implementazione di pipeline di elaborazione di flussi.&#x20;

Ci sono alcune caratteristiche di Kafka che vale la pena evidenziare. Innanzitutto, è progettato per una scala molto ampia: è stato sviluppato in LinkedIn per sostituire molteplici cluster di messaggi esistenti con una singola piattaforma ed è costruito per consentire la presenza di più consumers e producers.&#x20;

Un'altra caratteristica piuttosto unica di Kafka è la persistenza dei messaggi. Con un message broker normale, una volta che l'ultimo consumatore ha ricevuto un messaggio, l'intermediario non ha più bisogno di conservare quel messaggio. Con Kafka, i messaggi possono essere conservati per un periodo configurabile. Ciò significa che i messaggi possono essere conservati potenzialmente all'infinito. Ciò consente ai consumers di reinserire i messaggi che avevano già elaborato o consente ai consumers appena implementati di elaborare messaggi inviati in precedenza.

Infine, Kafka ha introdotto il supporto integrato per l'elaborazione di flussi quindi invece di utilizzare Kafka per inviare messaggi a uno strumento di elaborazione di flussi dedicato, alcune attività possono essere svolte all'interno di Kafka stesso.&#x20;





